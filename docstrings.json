{
    "correlation_using_db.py": {},
    "extract_docstrings.py": {},
    "train_using_csv.py": {},
    "train_using_db.py": {},
    "arima_model.py": {
        "get_arima_model": {
            "description": "Fit an ARIMA model to a time series and make predictions.\n\n    Args:\n        dataframe (pd.DataFrame): The input DataFrame containing time series data.\n        column_to_predict (str): The name of the column in the DataFrame to predict.\n        depth (int, optional): The number of time steps to forecast into the future. Default is 50.\n\n    Returns:\n        statsmodels.tsa.arima_model.ARIMAResultsWrapper: The trained ARIMA model.\n\n    This function fits an ARIMA (AutoRegressive Integrated Moving Average) model to a given time series data.\n    It then makes predictions for the specified number of time steps into the future and visualizes the results.\n\n    Example:\n        To fit an ARIMA model and make predictions:\n        df = pd.DataFrame(...)  # Your time series data DataFrame\n        column_name = 'Price'  # The column to predict\n        depth = 50  # Number of time steps to forecast\n        arima_model = get_arima_model(df, column_name, depth)"
        }
    },
    "corr.py": {
        "get_portfolio": {
            "description": "Calculate portfolio statistics based on correlations.\n\n    Args:\n        dataframe (pd.DataFrame): The input DataFrame containing historical data for assets.\n        columns_to_evaluate (list): A list of column names (assets) to evaluate.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the optimized portfolio weights for each asset.\n        float: The total correlation percent.\n\n    This function calculates portfolio statistics based on correlations between assets in the provided DataFrame.\n    It calculates the optimized portfolio weights for each asset and the total correlation percent.\n\n    Example:\n        To calculate portfolio statistics:\n        df = pd.DataFrame(...)  # Your historical data DataFrame\n        assets = ['Asset1', 'Asset2', 'Asset3']  # List of asset names\n        portfolio_weights, total_corr_percent = get_portfolio(df, assets)"
        }
    },
    "indicators.py": {},
    "models.py": {},
    "model_handler.py": {
        "__init__": {
            "description": "Initializes a ModelEvaluator object with default parameters and settings."
        },
        "_df_to_sequences_targets": {
            "description": "Convert a DataFrame into sequences and targets for training and testing.\n\n        Args:\n            dataframe (pd.DataFrame): The DataFrame containing the data.\n            columns_for_sequences (list): List of column names used for creating sequences.\n            test_only (bool): If True, only create sequences for testing data.\n\n        Returns:\n            tuple: A tuple containing train sequences, train targets, test sequences, and test targets.\n                If test_only is True, train sequences and train targets will be None.\n\n        Note:\n            This method converts the input DataFrame into sequences of data and their corresponding targets.\n            If test_only is False, it splits the data into training and testing sets based on the configured\n            training and validation proportions. The sequences are created with a sliding window approach,\n            where each sequence contains window_size data points and the target is the value at the next time step."
        },
        "_df_scale_and_clean": {
            "description": "Scale and clean the specified columns of a DataFrame.\n\n        Args:\n            dataframe (pd.DataFrame): The DataFrame containing the data to be scaled and cleaned.\n            columns_for_sequences (list): List of column names to be scaled and cleaned.\n\n        Returns:\n            pd.DataFrame: The DataFrame with specified columns scaled and cleaned.\n\n        Note:\n            This method performs two main data preprocessing steps:\n            1. It fills missing values (NaNs) in the specified columns using the mean value of the column.\n            2. It normalizes the data in the specified columns using the Min-Max scaling technique, which scales\n               the data to the range [0, 1]."
        },
        "csv_load": {
            "description": "Load data from a CSV file and prepare it for model evaluation.\n\n        Args:\n            file_path (str): The path to the CSV file containing the data.\n            date_time_column_name (str): The name of the column containing date-time information.\n            target_column_name (str): The name of the target column to predict.\n            features_column_names (list, optional): A list of column names representing features.\n            item_ids_column_name (int, optional): The name or index of the column containing item IDs.\n\n        Returns:\n            None\n\n        Note:\n            This method loads data from a CSV file, performs preprocessing steps, and prepares the data for model evaluation.\n            It handles date-time parsing, duplicate removal, and sorting of item IDs if item_ids_column_name is provided."
        },
        "db_connect": {
            "description": "Establish a connection to a PostgreSQL database.\n\n        Args:\n            host (str): The host address of the PostgreSQL server.\n            port (str): The port number for the PostgreSQL server.\n            db_name (str): The name of the database to connect to.\n            user_name (str): The username for authentication.\n            password (str): The password for authentication.\n\n        Returns:\n            None\n\n        Note:\n            This method establishes a connection to a PostgreSQL database using the provided connection details\n            such as host, port, database name, username, and password. It also initializes a database cursor for executing\n            SQL queries."
        },
        "_db_get_item_count_by_query": {
            "description": "Retrieve the count of database items using a SQL query.\n\n        Args:\n            query (str): The SQL query to execute for counting items.\n\n        Returns:\n            None\n\n        Note:\n            This method executes a SQL query to count items in the database and updates the `cycle_items_ids` and `cycle_size`\n            attributes based on the query result."
        },
        "_db_get_item_by_month": {
            "description": "Retrieve data for a specific item within a date range from the database.\n\n        Args:\n            item_id (int): The ID of the item for which to retrieve data.\n            date_start: The start date of the date range.\n            date_end: The end date of the date range.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the retrieved data.\n\n        Note:\n            This method executes a SQL query to retrieve data for a specific item within the specified date range\n            from the database. The query is constructed using the provided item ID, start date, and end date. The retrieved\n            data is then converted into a DataFrame with column names based on `column_format`."
        },
        "_db_transform_column_data_types": {
            "description": "Transform the data types of columns in the DataFrame to match the specified data types.\n\n        Args:\n            dataframe (pd.DataFrame): The DataFrame containing the data to be transformed.\n\n        Returns:\n            pd.DataFrame: A DataFrame with transformed data types.\n\n        Note:\n            This method iterates through the columns of the DataFrame and converts the data types of specific columns\n            based on the `column_format` attribute. It ensures that date columns are converted to datetime objects\n            and other columns are converted to the specified data types. The resulting DataFrame is set to have\n            a datetime index and is cast to 'float32' data type."
        },
        "_add_indicators": {
            "description": "Add indicator columns to the DataFrame based on the target column.\n\n        Args:\n            dataframe (pd.DataFrame): The DataFrame to which indicators will be added.\n\n        Returns:\n            pd.DataFrame: The DataFrame with added indicator columns.\n            list: A list of column names, including the target and indicator columns.\n\n        Note:\n            This method calculates and adds indicator columns to the DataFrame based on the target column's data.\n            It uses the `get_indicators` function to generate the indicators, appends them to the DataFrame, and\n            returns both the updated DataFrame and a list of column names, including the target and indicator columns."
        },
        "_year_month_day_plus_months_to_date_times": {
            "description": "Calculate the start and end dates based on a given year and month.\n\n        Args:\n            year (int): The starting year.\n            month (int): The starting month (1-12).\n            months (int, optional): The number of months to add to the start date (default is 1).\n\n        Returns:\n            tuple: A tuple containing the start date and end date as datetime.date objects.\n\n        Raises:\n            ValueError: If the `months` argument is less than 1.\n\n        Note:\n            This method takes a starting year and month, and optionally a number of months to add.\n            It calculates the start date as the first day of the specified month and the end date by adding the\n            specified number of months to the start date. The result is returned as a tuple of datetime.date objects.\n            If `months` is less than 1, a ValueError is raised."
        },
        "_date_parser": {
            "description": "Parse a string into a date using Pandas Timestamp.\n\n        Args:\n            string (str): The input string representing a date.\n\n        Returns:\n            datetime.date: A date object extracted from the input string.\n\n        Note:\n            This static method takes an input string representing a date and parses it using Pandas Timestamp.\n            It returns the parsed date as a `datetime.date` object."
        },
        "_create_sequences": {
            "description": "Create sequences and corresponding targets from input data.\n\n        Args:\n            data (pd.DataFrame): The input data frame.\n\n        Returns:\n            tuple: A tuple containing sequences and targets as NumPy arrays.\n\n        Note:\n            This method takes input data and creates sequences and corresponding targets based on the window size\n            and window size of indicators. It iterates through the data frame and extracts sequences of data\n            and their corresponding target values. The sequences and targets are returned as NumPy arrays."
        },
        "_get_minimum_frametime_length": {
            "description": "Calculate the minimum frame time length for validation.\n\n        Returns:\n            int: The minimum frame time length.\n\n        Note:\n            This method calculates the minimum frame time length based on the window size, window size of indicators,\n            and the validation part. The result is an integer representing the minimum frame time length."
        },
        "get_risk_coefficient": {
            "description": "Calculate the risk coefficient based on item IDs and a specified column.\n\n        Args:\n            item_ids (list): A list of item IDs for which to calculate the risk coefficient.\n            column_name (str): The name of the column used for risk calculation.\n            start_year (int): The starting year for data retrieval.\n            start_month (int): The starting month for data retrieval.\n            months (int): The number of months for data retrieval.\n            start_date (datetime.date): The specific starting date for data retrieval.\n            end_date (datetime.date): The specific ending date for data retrieval.\n\n        Note:\n            This method calculates the risk coefficient for a list of item IDs using a specified column of data.\n            It allows for various options for specifying the time period for data retrieval, including start_year and\n            months, or start_date and end_date."
        },
        "start_training": {
            "description": "Start the training process for a machine learning model.\n\n        Args:\n            model (tf.keras.Model): The machine learning model to train.\n            epochs (int): The number of training epochs.\n            item_ids (list): A list of item IDs to train on. If provided, training is performed on these items only.\n            start_year (int): The starting year for data retrieval.\n            start_month (int): The starting month for data retrieval.\n            months (int): The number of months for data retrieval.\n            start_date (datetime.date): The specific starting date for data retrieval.\n            end_date (datetime.date): The specific ending date for data retrieval.\n            use_arima (bool): If True, use ARIMA modeling instead of machine learning model training.\n            use_indicators (bool): If True, use Indicators for machine learning model training. Not work with ARIMA.\n\n        Note:\n            This method initiates the training process for a machine learning model, allowing for various options\n            such as specifying the time period for data retrieval, using ARIMA modeling, and training on specific item IDs.\n            If item_ids are provided, training is performed only on those items. If use_arima is True, ARIMA modeling is\n            used instead of machine learning model training."
        },
        "_prepare_and_remember_dataframe_arima": {
            "description": "Prepare and remember the DataFrame for ARIMA modeling.\n\n        Args:\n            dataframe (pd.DataFrame): The input DataFrame containing the data for modeling.\n\n        This method applies data preprocessing steps such as normalization and handling missing values to prepare\n        the DataFrame for use with ARIMA modeling. It focuses on a single target column specified by 'self.target_column_name'.\n        After preprocessing, the DataFrame is stored in 'self.dataframe' for use in modeling.\n\n        Note:\n            The 'self.target_column_name' attribute should be set to specify the target column for ARIMA modeling."
        },
        "_prepare_and_remember_dataframe": {
            "description": "Prepare and remember the DataFrame for machine learning model training.\n\n        Args:\n            dataframe (pd.DataFrame): The input DataFrame containing the data for training.\n\n        This method performs the following steps to prepare the DataFrame for training with a machine learning model:\n        1. Builds indicators based on the current dataframe and adds them to the dataframe.\n        2. Applies data preprocessing steps such as normalization, handling missing values, and cleaning correlations.\n        3. Extracts sequences of features and targets from the dataframe.\n\n        After preprocessing and feature extraction, the resulting sequences and targets are stored in instance variables\n        for use in model training. The prepared dataframe is also stored in 'self.dataframe', and the list of columns\n        used for training is stored in 'self.df_columns_to_train'.\n\n        Note:\n            The target column for training should be specified in the 'self.target_column_name' attribute."
        },
        "_get_estimate_and_update_timer": {
            "description": "Calculate the estimated time remaining for the current task and update the timer.\n\n        Returns:\n            datetime.timedelta: The estimated time remaining for the current task.\n\n        This method calculates the estimated time remaining for processing items in a cycle and updates the timer\n        for tracking task progress. It does so by comparing the current time with the time of the last cycle update.\n        The estimated time remaining is based on the time taken for processing previous items in the cycle and\n        extrapolates it for the remaining items.\n\n        The calculated estimated time remaining is returned as a `datetime.timedelta` object.\n\n        Note:\n            This method is typically used to provide progress updates during a long-running task."
        },
        "db_test_model_on_item": {
            "description": "Test a machine learning model on a specific item's data from a database.\n\n        Args:\n            item_id (int): The ID of the item to test the model on.\n            start_year (int): The start year for fetching data.\n            start_month (int): The start month (default: 1).\n            months (int): The number of months to fetch data (default: 1).\n            start_date (datetime.date): The specific start date for data fetching (optional).\n            end_date (datetime.date): The specific end date for data fetching (optional).\n\n        This method retrieves data for a specific item from a database, prepares it for testing, and evaluates the model's accuracy\n        on this data. It performs the following steps:\n\n        1. Fetches raw data for the specified item from the database based on the provided date range.\n        2. Checks if the data is long enough for testing; if not, it skips testing for this item.\n        3. Converts the data types of the dataframe for compatibility with TensorFlow.\n        4. Builds indicators based on the current dataframe and adds them to the dataframe.\n        5. Applies data preprocessing steps such as normalization, handling missing values, and cleaning correlations.\n        6. Creates sequences and targets for testing using the prepared dataframe.\n        7. Loads the machine learning model previously trained.\n        8. Evaluates the accuracy of the model on the test data.\n\n        Note:\n            If the data for the specified item is too short to test, it will be skipped, and a message will be printed."
        },
        "_text_lvl": {
            "description": "Returns text formatted as a process level line.\n\n        Args:\n            lvl_max (int): The maximum level of the process.\n            lvl_cur (int): The current level of the process.\n\n        Returns:\n            str: A text line representing the process level, with '=' characters indicating the current level\n                 and '-' characters indicating the remaining level.\n\n        This static method generates a text line to visually represent the progress or level of a process.\n        It creates a line of characters where '=' characters represent the current level, and '-' characters represent\n        the remaining level up to the maximum level.\n\n        Example:\n            If lvl_max=5 and lvl_cur=3, the method will return '===--', indicating that the process is at level 3\n            out of a maximum of 5 levels."
        },
        "_start_training": {
            "description": "Train a TensorFlow Keras model.\n\n        Args:\n            model (tf.keras.Model): The Keras model to be trained.\n\n        Returns:\n            tf.keras.Model: The trained Keras model.\n\n        This method compiles and trains the provided Keras model. It compiles the model with a specified loss function\n        (Mean Squared Error), optimizer (Adam optimizer with a learning rate of 0.0005), and metrics (Mean Absolute Error\n        and Accuracy). The training process includes multiple epochs with specified training parameters, and the training\n        history is stored.\n\n        The model file name for saving is determined based on whether there are item IDs or a cycle size. Metrics from\n        multiple training runs can be concatenated for analysis. Memory management is performed after each training run.\n\n        Example:\n            To train a model, you can call this method with the desired Keras model:\n            model = MyKerasModel()\n            trained_model = self._start_training(model=model)"
        },
        "_print_training_log": {
            "description": "Print training progress log during model training.\n\n        Args:\n            epoch (int): The current training epoch.\n            logs (dict): A dictionary containing training metrics.\n\n        This method prints a training log to the console, providing information about the current training progress. It\n        displays the current item number, the total number of items, the current epoch, the total number of epochs, the\n        estimated time remaining for the training, the Mean Absolute Error (MAE), and the loss.\n\n        Example:\n            This method is typically called during model training to provide real-time training progress updates.\n            model = MyKerasModel()\n            history = model.fit(...)\n            for epoch in range(epochs):\n                self._print_training_log(epoch, history.history)"
        },
        "plot_accuracy": {
            "description": "Plot the training and validation Mean Absolute Error (MAE) over epochs.\n\n        Args:\n            save_file (bool, optional): Whether to save the generated plot as an image file. Default is True.\n\n        This method plots the Mean Absolute Error (MAE) for both the training and validation datasets over epochs during\n        model training. It can help visualize the model's training progress and identify potential overfitting or\n        underfitting.\n\n        Example:\n            After training a model, you can use this method to plot and visualize the training and validation MAE trends.\n            trained_model = MyTrainedModel()\n            trained_model.load_weights('my_model_weights.h5')\n            trained_model.compile(...)\n            trained_model.plot_accuracy(save_file=True)"
        },
        "_predict": {
            "description": "Generate predictions for future time steps using the trained model.\n\n        Parameters:\n        - horizon (int): The number of future time steps to forecast.\n        - model (keras.Model): The trained Keras model for making predictions.\n\n        Returns:\n        - prediction_list (np.ndarray): An array containing the forecasted values for the specified horizon."
        },
        "_predict_dates": {
            "description": "Generate dates for future predictions based on the last date in the test data.\n\n        Parameters:\n        - num_prediction (int): The number of future time steps to forecast.\n\n        Returns:\n        - prediction_dates (list of datetime.date): A list of datetime dates representing the forecasted dates."
        },
        "forecast": {
            "description": "Generate forecasts for future data points using the trained model.\n\n        Parameters:\n        - model: A trained machine learning model.\n        - horizon (int, optional): The number of future time steps to forecast. Defaults to 30.\n\n        Returns:\n        - forecasted_dataframe (pd.DataFrame): A DataFrame containing the forecasted data with dates as the index."
        },
        "plot_prediction": {
            "description": "Plot the model's predictions for future data points.\n\n        Parameters:\n        - save_file (bool, optional): Whether to save the plot as an image file. Defaults to True.\n\n        Returns:\n        None"
        },
        "_save_image": {
            "description": "Save the current matplotlib plot as an image file.\n\n        Args:\n            prefix (str, optional): A prefix to include in the image file name. Default is 'nan'.\n\n        This method saves the current matplotlib plot as an image file (JPEG format) in the specified\n        output directory. It also includes a timestamp in the file name to make it unique.\n\n        Example:\n            To save the current plot as an image file:\n            my_instance = MyInstance()\n            my_instance._save_image(prefix='plot')"
        },
        "_save_model": {
            "description": "Save a TensorFlow/Keras model to a file.\n\n        Args:\n            model (tf.keras.Model): The TensorFlow/Keras model to be saved.\n            file_name (str): The name of the file to save the model to.\n\n        This method saves a trained TensorFlow/Keras model to the specified file path. The model can be loaded later\n        for reuse or further evaluation.\n\n        Example:\n            To save a trained model to a file:\n            my_instance = MyInstance()\n            my_model = tf.keras.Sequential([...])  # Define and train the model\n            my_instance._save_model(my_model, file_name='my_model.h5')"
        },
        "_load_model": {
            "description": "Load a TensorFlow/Keras model from a file.\n\n        Args:\n            file_name (str): The name of the file containing the saved model.\n\n        Returns:\n            tf.keras.Model or None: The loaded TensorFlow/Keras model if successful, or None if loading failed.\n\n        This method attempts to load a saved TensorFlow/Keras model from the specified file path. If the model is successfully\n        loaded, it is returned. If loading fails, None is returned.\n\n        Example:\n            To load a previously saved model from a file:\n            my_instance = MyInstance()\n            loaded_model = my_instance._load_model(file_name='my_model.h5')\n            if loaded_model:\n                # Use the loaded model for inference or further training\n            else:\n                print(\"Model loading failed.\")"
        },
        "prepare_and_stop": {
            "description": "Close database connection and exit the program with an error message.\n\n        Args:\n            message (str): The error message to display before exiting.\n\n        This method closes the database connection (if open) and then exits the program with an error message. It's typically\n        used to gracefully handle errors and terminate the program when necessary.\n\n        Example:\n            To close the database connection and exit the program with an error message:\n            my_instance = MyInstance()\n            my_instance.prepare_and_stop(\"An error occurred. Exiting.\")"
        }
    },
    "__init__.py": {}
}